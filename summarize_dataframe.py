import hashlib
from typing import Any, Dict, List

from pyspark.sql import DataFrame, SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType


def summarize_dataframe(
    dataframe: DataFrame,
    binary_variables: List[str] = None,
    categorical_variables: List[str] = None,
    numerical_variables: List[str] = None,
) -> Dict[str, Any]:
    """
    Scans a PySpark DataFrame and creates a summary dictionary of its statistical properties.

    Args:
        dataframe: The input PySpark DataFrame.
        binary_variables: A list of column names with binary values (e.g., 0/1, True/False).
        categorical_variables: A list of column names with categorical values.
        numerical_variables: A list of column names with numerical values.

    Returns:
        A dictionary containing the schema and statistical summaries for each variable type.
    """
    binary_variables = binary_variables or []
    categorical_variables = categorical_variables or []
    numerical_variables = numerical_variables or []
    
    all_vars = binary_variables + categorical_variables + numerical_variables
    schema_dict = {
        field.name: field.dataType.simpleString()
        for field in dataframe.schema if field.name in all_vars
    }

    summary = {
        "schema": schema_dict,
        "binary_variables": {},
        "categorical_variables": {},
        "numerical_variables": {},
    }

    total_count = dataframe.count()
    if total_count == 0:
        print("Warning: Input DataFrame is empty. Returning an empty summary.")
        return summary

    # --- Process Binary Variables ---
    for col_name in binary_variables:
        if col_name not in dataframe.columns:
            print(f"Warning: Binary column '{col_name}' not found. Skipping.")
            continue
            
        counts = dataframe.groupBy(col_name).count().collect()
        if len(counts) != 2:
            print(f"Warning: Column '{col_name}' has {len(counts)} unique values, not 2. Skipping.")
            continue
            
        # Sort by value to ensure deterministic order
        counts.sort(key=lambda row: row[col_name])
        val1, val2 = counts[0][col_name], counts[1][col_name]
        ratio = counts[1]["count"] / total_count
        summary["binary_variables"][col_name] = {"ratio": ratio, "values": [val1, val2]}

    # --- Process Categorical Variables ---
    for col_name in categorical_variables:
        if col_name not in dataframe.columns:
            print(f"Warning: Categorical column '{col_name}' not found. Skipping.")
            continue

        counts_df = dataframe.groupBy(col_name).count()
        proportions = {
            row[col_name]: row["count"] / total_count for row in counts_df.collect()
        }
        summary["categorical_variables"][col_name] = {"proportions": proportions}

    # --- Process Numerical Variables ---
    if numerical_variables:
        valid_numerical_vars = [v for v in numerical_variables if v in dataframe.columns]
        if not valid_numerical_vars:
             print("Warning: No valid numerical variables found in DataFrame.")
        else:
            agg_exprs = []
            for col_name in valid_numerical_vars:
                agg_exprs.extend([
                    F.mean(col_name).alias(f"{col_name}_mean"),
                    F.stddev(col_name).alias(f"{col_name}_stddev"),
                    F.min(col_name).alias(f"{col_name}_min"),
                    F.max(col_name).alias(f"{col_name}_max"),
                ])
            
            stats_row = dataframe.agg(*agg_exprs).first()
            for col_name in valid_numerical_vars:
                summary["numerical_variables"][col_name] = {
                    "mean": stats_row[f"{col_name}_mean"],
                    "stddev": stats_row[f"{col_name}_stddev"],
                    "min": stats_row[f"{col_name}_min"],
                    "max": stats_row[f"{col_name}_max"],
                }

    return summary


def create_mock_dataframe(
    spark: SparkSession, summary_dict: Dict[str, Any], n: int
) -> DataFrame:
    """
    Creates a deterministic mock PySpark DataFrame based on a summary dictionary.

    Args:
        spark: The active SparkSession.
        summary_dict: The dictionary generated by `summarize_dataframe`.
        n: The desired number of rows for the mock DataFrame.

    Returns:
        A mock PySpark DataFrame that statistically resembles the original data.
    """
    if n <= 0:
        print("Warning: n <= 0, returning an empty DataFrame.")
        return spark.createDataFrame([], StructType([]))

    df = spark.range(n)

    def _get_seed_from_string(s: str) -> int:
        """Creates a deterministic integer seed from a string."""
        return int(hashlib.sha256(s.encode("utf-8")).hexdigest(), 16) % (10**9)

    schema = summary_dict.get("schema", {})

    # --- Generate Binary Variables ---
    for col_name, stats in summary_dict.get("binary_variables", {}).items():
        seed = _get_seed_from_string(col_name)
        ratio = stats["ratio"]
        # values are sorted [val1, val2] from summary function
        positive_val, negative_val = stats["values"][1], stats["values"][0] 
        df = df.withColumn(
            col_name,
            F.when(F.rand(seed) < ratio, positive_val).otherwise(negative_val),
        )

    # --- Generate Categorical Variables ---
    for col_name, stats in summary_dict.get("categorical_variables", {}).items():
        seed = _get_seed_from_string(col_name)
        rand_col = F.rand(seed)
        proportions = stats["proportions"]
        
        # Sort categories by name for a deterministic 'when' chain
        sorted_categories = sorted(proportions.items(), key=lambda item: item[0])
        
        when_chain = None
        cumulative_prob = 0.0
        # Build a chain of WHEN clauses to sample from the distribution
        for category, prob in sorted_categories[:-1]:
            cumulative_prob += prob
            if when_chain is None:
                when_chain = F.when(rand_col < cumulative_prob, category)
            else:
                when_chain = when_chain.when(rand_col < cumulative_prob, category)
        
        last_category = sorted_categories[-1][0]
        when_chain = when_chain.otherwise(last_category)
        df = df.withColumn(col_name, when_chain)

    # --- Generate Numerical Variables ---
    for col_name, stats in summary_dict.get("numerical_variables", {}).items():
        seed = _get_seed_from_string(col_name)
        mean, stddev = stats["mean"], stats.get("stddev")
        min_val, max_val = stats["min"], stats["max"]
        
        if stddev is None or stddev == 0 or mean is None:
            # If no variance or mean, all values are constant
            generated_col = F.lit(mean if mean is not None else 0)
        else:
            # Generate from a normal distribution and scale
            base_col = (F.randn(seed) * stddev) + mean
            # Clip the values to respect the original min/max bounds
            generated_col = F.greatest(F.lit(min_val), F.least(F.lit(max_val), base_col))

        df = df.withColumn(col_name, generated_col)
        
    # --- Cast all generated columns to their original data types ---
    for col_name, spark_type in schema.items():
        if col_name in df.columns:
            df = df.withColumn(col_name, F.col(col_name).cast(spark_type))
            
    # Select columns in a sorted, deterministic order
    final_cols = sorted(schema.keys())
    return df.select(final_cols)


if __name__ == "__main__":
    # --- Example Usage ---
    spark = SparkSession.builder.appName("MockDataFrameGenerator").getOrCreate()

    # 1. Create a sample original DataFrame
    data = [
        (1, "A", 10.5, True, 100),
        (0, "B", 12.3, False, 120),
        (1, "A", 11.8, False, 110),
        (1, "C", 9.1, True, 95),
        (0, "B", 15.2, True, 155),
        (1, "A", 10.9, False, 105),
    ]
    columns = ["binary_feat", "category_feat", "numeric_feat1", "bool_feat", "numeric_feat2"]
    original_df = spark.createDataFrame(data, columns)
    
    print("--- Original DataFrame ---")
    original_df.show()
    original_df.printSchema()

    # 2. Summarize the DataFrame into a dictionary
    summary = summarize_dataframe(
        dataframe=original_df,
        binary_variables=["binary_feat", "bool_feat"],
        categorical_variables=["category_feat"],
        numerical_variables=["numeric_feat1", "numeric_feat2"],
    )
    
    import json
    print("\n--- Summary Dictionary ---")
    print(json.dumps(summary, indent=2))

    # 3. Create a new mock DataFrame from the summary
    mock_df = create_mock_dataframe(spark=spark, summary_dict=summary, n=10)

    print("\n--- Generated Mock DataFrame (n=10) ---")
    mock_df.show()
    mock_df.printSchema()

    # You can verify that running this multiple times produces the exact same mock DataFrame
    print("\n--- Verifying Determinism (Another Run) ---")
    mock_df_run2 = create_mock_dataframe(spark=spark, summary_dict=summary, n=10)
    mock_df_run2.show()
    
    spark.stop()
