# engine.py

import polars as pl
import numpy as np
import warnings
import matplotlib.pyplot as plt

from simulation.engine_utils import create_train_val_test_split_series
from .population import Population
from .policy import Policy
from .ground_truth import calculate_rewards, calculate_total_reward


class Engine:
    def __init__(self, population: Population, choosing_policy: Policy):
        self.population = population
        self.choosing_policy = choosing_policy
        self.rewards = []  # Changed to a list to append rewards easily

    def evolve(self, n_steps=25):
        """Evolve the population n_steps using the specified choosing policy"""
        print(
            f"Starting simulation for {n_steps} steps with {type(self.choosing_policy).__name__}."
        )
        for t in range(n_steps):
            print(f"Simulating step {t+1}/{n_steps}...")

            # 2. Policy makes a choice based on the current state
            # The policy's choose method should return a Polars Series of chosen action names
            chosen_actions_series = self.choosing_policy.choose(self.population)

            # 3. Update the population's state with the chosen actions
            self.population.update_state_with_actions(chosen_actions_series)

            # For evaluation and training purposes, assign a train/test/validation split to population
            create_train_val_test_split_series(
                self.population, test_size=0.30, validation_size=0.00
            )

            # 4. Simulate the outcome of this choice for the next time step
            # This function now encapsulates the calculation of logits, probabilities,
            # and the Bernoulli draw, updating the product possession state internally.
            # It returns the binary outcome for the current time step.
            product_outcome_df = self.population.simulate_product_possession_outcome(
                seed=t,  # Use time step as seed for reproducibility of each step
            )

            # 5. Extract and collect the reward generated by our choice
            # Calculate rewards based on the product_outcome_df (binary 0/1 purchases)
            individual_rewards = calculate_rewards(
                product_outcome_df, self.population.config
            ).alias("rewards")

            total_reward_this_step = calculate_total_reward(individual_rewards)
            self.rewards.append(total_reward_this_step)
            print(f"Step {t+1} completed. Total reward: {total_reward_this_step:.2f}")

        print("Simulation complete.")

    def plot_rewards(self):
        """Plot the accumulated reward each time step"""
        if not self.rewards:
            print("No rewards to plot. Run the evolve method first.")
            return

        plt.figure(figsize=(10, 6))
        plt.plot(
            range(1, len(self.rewards) + 1), self.rewards, marker="o", linestyle="-"
        )
        plt.title(f"Total Reward Over Time ({type(self.choosing_policy).__name__})")
        plt.xlabel("Simulation Step")
        plt.ylabel("Total Reward")
        plt.grid(True)
        plt.show()

    def get_population_state(self) -> pl.DataFrame:
        """
        Returns the state of the population DataFrame after N simulation steps.
        """
        return self.population.get_state()

    def get_population(self) -> Population:
        """
        Returns the population DataFrame after N simulation steps.
        """
        return self.population
